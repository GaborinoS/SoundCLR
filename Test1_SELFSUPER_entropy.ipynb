{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./results/2023-09-21_crossEntropyLoss/classifier already exists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from models import model_classifier\n",
    "from utils_dir.utils import *\n",
    "import config\n",
    "\n",
    "if config.ESC_10:\n",
    "    import dataset_ESC10 as dataset\n",
    "elif config.ESC_50:\n",
    "    import dataset_ESC50 as dataset\n",
    "elif config.US8K:\n",
    "    import dataset_US8K as dataset\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Update the model loading line\n",
    "model = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "model.fc = nn.Sequential(nn.Identity())\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0])\n",
    "model = model.to(device)\n",
    "\n",
    "classifier = model_classifier.Classifier().to(device)\n",
    "\n",
    "train_loader, val_loader = dataset.create_generators()\n",
    "\n",
    "root = './results/'\n",
    "main_path = root + str(datetime.datetime.now().strftime('%Y-%m-%d')) + \"_crossEntropyLoss\"\n",
    "if not os.path.exists(main_path):\n",
    "    os.mkdir(main_path)\n",
    "\n",
    "classifier_path = main_path + '/' + 'classifier'\n",
    "\n",
    "# Modify the code that creates directories to handle existing directories\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.mkdir(classifier_path)\n",
    "else:\n",
    "    print(f\"Directory {classifier_path} already exists.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(classifier.parameters()),\n",
    "                             lr=config.lr, weight_decay=1e-3)\n",
    "\n",
    "scheduler = WarmUpExponentialLR(optimizer, cold_epochs=0, warm_epochs=config.warm_epochs, gamma=config.gamma)\n",
    "\n",
    "\n",
    "def hotEncoder(v):\n",
    "    ret_vec = torch.zeros(v.shape[0], config.class_numbers).to(device)\n",
    "    for s in range(v.shape[0]):\n",
    "        ret_vec[s][v[s]] = 1\n",
    "    return ret_vec\n",
    "\n",
    "def cross_entropy_one_hot(input, target):\n",
    "    _, labels = target.max(dim=1)\n",
    "    return nn.CrossEntropyLoss(weight=class_weights)(input, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(config.ESC_50)\n",
    "print(config.ESC_10)\n",
    "print(config.US8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###########################################################################################\n",
    "#create class weight vector with the length of number of classes with 0\n",
    "class_weights = torch.ones(config.class_numbers).to(device)\n",
    "\n",
    "main_model = model.module if hasattr(model, 'module') else model\n",
    "###########################################################################################\n",
    "\n",
    "def train_crossEntropy():\n",
    "\tnum_epochs = 800\n",
    "\twith open(main_path + '/results.txt','w', 1) as output_file:\n",
    "\t\tmainModel_stopping = EarlyStopping(patience=300, verbose=True, log_path=main_path, output_file=output_file)\n",
    "\t\tclassifier_stopping = EarlyStopping(patience=300, verbose=False, log_path=classifier_path, output_file=output_file)\n",
    "\n",
    "\t\tprint('*****', file=output_file)\n",
    "\t\tprint('BASELINE', file=output_file)\n",
    "\t\tprint('transfer - augmentation on both waves and specs - 3 channels', file=output_file)\n",
    "\t\tif config.ESC_10:\n",
    "\t\t\tprint('ESC_10', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.train_folds, config.test_fold), file=output_file)\n",
    "\t\telif config.ESC_50:\n",
    "\t\t\tprint('ESC_50', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.train_folds, config.test_fold), file=output_file)\n",
    "\t\telif config.US8K:\n",
    "\t\t\tprint('US8K', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.us8k_train_folds, config.us8k_test_fold), file=output_file)\n",
    "\n",
    "\n",
    "\t\tprint('number of freq masks are {} and their max length is {}'.format(config.freq_masks, config.freq_masks_width), file=output_file)\n",
    "\t\tprint('number of time masks are {} and their max length is {}'.format(config.time_masks, config.time_masks_width), file=output_file)\n",
    "\t\tprint('*****', file=output_file)\n",
    "\t\n",
    "\n",
    "\n",
    "\t\tfor epoch in range(num_epochs):\n",
    "\t\t\tmodel.train()\n",
    "\t\t\tclassifier.train()\n",
    "        \n",
    "\t\t\ttrain_loss = []\n",
    "\t\t\ttrain_corrects = 0\n",
    "\t\t\ttrain_samples_count = 0\n",
    "        \n",
    "\t\t\tfor _, x, label in train_loader:\n",
    "\t\t\t\tloss = 0\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "            \n",
    "\t\t\t\tinp = x.float().to(device)\n",
    "\t\t\t\tlabel = label.to(device).unsqueeze(1)\n",
    "\t\t\t\tlabel_vec = hotEncoder(label)\n",
    "            \n",
    "\t\t\t\ty_rep = model(inp)\n",
    "\t\t\t\ty_rep = F.normalize(y_rep, dim=0)\n",
    "            \n",
    "\t\t\t\ty_pred = classifier(y_rep)\n",
    "            \n",
    "\t\t\t\tloss += cross_entropy_one_hot(y_pred, label_vec)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\ttrain_loss.append(loss.item() )\n",
    "\t\t\t\toptimizer.step()\n",
    "            \n",
    "\t\t\t\ttrain_corrects += (torch.argmax(y_pred, dim=1) == torch.argmax(label_vec, dim=1)).sum().item()\n",
    "\t\t\t\ttrain_samples_count += x.shape[0]\n",
    "        \n",
    "        \n",
    "\t\t\tval_loss = []\n",
    "\t\t\tval_corrects = 0\n",
    "\t\t\tval_samples_count = 0\n",
    "        \n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tclassifier.eval()\n",
    "        \n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor _, val_x, val_label in val_loader:\n",
    "\t\t\t\t\tinp = val_x.float().to(device)\n",
    "\t\t\t\t\tlabel = val_label.to(device)\n",
    "\t\t\t\t\tlabel_vec = hotEncoder(label)\n",
    "                \n",
    "\t\t\t\t\ty_rep = model(inp)\n",
    "\t\t\t\t\ty_rep = F.normalize(y_rep, dim=0)\n",
    "\n",
    "\t\t\t\t\ty_pred = classifier(y_rep)\n",
    "                \n",
    "\t\t\t\t\ttemp = cross_entropy_one_hot(y_pred, label_vec)\n",
    "\t\t\t\t\tval_loss.append(temp.item() )\n",
    "                \n",
    "\t\t\t\t\tval_corrects += (torch.argmax(y_pred, dim=1) == torch.argmax(label_vec, dim=1)).sum().item() \n",
    "\t\t\t\t\tval_samples_count += val_x.shape[0]\n",
    "        \n",
    "\t\t\n",
    "        \n",
    "\t\t\tscheduler.step()\n",
    "        \n",
    "\t\t\ttrain_acc = train_corrects / train_samples_count\n",
    "\t\t\tval_acc = val_corrects / val_samples_count\n",
    "\t\t\tprint('\\n', file=output_file)\n",
    "\t\t\tprint(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \"Loss: {:.4f}...\".format(np.mean(train_loss)),\n",
    "\t\t\t\t\"Val Loss: {:.4f}\".format(np.mean(val_loss)), file=output_file)\n",
    "\t\t\tprint('train_acc is {:.4f} and val_acc is {:.4f}'.format(train_acc, val_acc), file=output_file)\n",
    "\t\t\tmainModel_stopping(-val_acc, main_model, epoch+1)\n",
    "\t\t\tclassifier_stopping(-val_acc, classifier, epoch+1)\n",
    "\t\t\tif mainModel_stopping.early_stop:\n",
    "\t\t\t\tprint(\"Early stopping\", file=output_file)\n",
    "\t\t\t\treturn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttrain_crossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the instantiation of the classifier object\n",
    "#classifier = Classifier().to(device) # Ensure Classifier class is defined before this line\n",
    "\n",
    "# Get the main model from the DataParallel module\n",
    "main_model = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# Load the checkpoints\n",
    "main_model.load_state_dict(torch.load('results/2023-09_crossEntropyLoss10_f/checkpoint.pt'))\n",
    "classifier.load_state_dict(torch.load('results/2023-09_crossEntropyLoss10_f/classifier/checkpoint.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.50%\n",
      "Precision: 89.94%\n",
      "Recall: 87.50%\n",
      "F1-score: 87.67%\n",
      "Confusion Matrix:\n",
      "[[6 0 0 0 0 0 2 0 0 0]\n",
      " [0 8 0 0 0 0 0 0 0 0]\n",
      " [0 0 6 2 0 0 0 0 0 0]\n",
      " [0 0 0 8 0 0 0 0 0 0]\n",
      " [0 0 0 0 8 0 0 0 0 0]\n",
      " [0 0 0 0 0 8 0 0 0 0]\n",
      " [0 0 0 0 0 1 7 0 0 0]\n",
      " [0 0 1 0 0 1 0 6 0 0]\n",
      " [0 0 1 0 0 0 0 0 7 0]\n",
      " [0 0 0 2 0 0 0 0 0 6]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "# Initialize variables to store the true and predicted labels\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "with torch.no_grad():\n",
    "    for _, x, label in val_loader:\n",
    "        inp = x.float().to(device)\n",
    "        label = label.to(device).unsqueeze(1)\n",
    "        label_vec = hotEncoder(label)\n",
    "        \n",
    "        y_rep = main_model(inp)\n",
    "        y_rep = F.normalize(y_rep, dim=0)\n",
    "\n",
    "        y_pred = classifier(y_rep)\n",
    "        \n",
    "        true_labels.extend(label.squeeze(1).cpu().numpy())\n",
    "        pred_labels.extend(torch.argmax(y_pred, dim=1).cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_preds = sum(t == p for t, p in zip(true_labels, pred_labels))\n",
    "accuracy = correct_preds / len(true_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score, and support\n",
    "precision, recall, f1_score, support = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_mat = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1-score: {f1_score * 100:.2f}%\")\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[0;32m     40\u001b[0m images, labels, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataiter)\n\u001b[1;32m---> 41\u001b[0m output_shape \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_conv_output_shape(images[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device))\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(output_shape)  \u001b[39m# Print the output shape\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m# Now update the input dimensions of your Linear layer and view function\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ensure you have your config file imported\n",
    "import config\n",
    "\n",
    "# Assuming your dataset script is named dataset.py\n",
    "#import dataset_US8K as downstream_dataset\n",
    "\n",
    "# Define a simple CNN model (replace this with your preferred model)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 63 * 127, 10)  # We will update this later\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 63 * 127)  # We will update this later\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def get_conv_output_shape(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        return x.shape\n",
    "\n",
    "# Initialize the model, define the loss function and the optimizer\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Create data generators\n",
    "train_loader, test_loader = dataset.create_generators()\n",
    "\n",
    "# Get the shape of the output after the conv and pool layers\n",
    "dataiter = iter(train_loader)\n",
    "images, labels, _ = next(dataiter)\n",
    "output_shape = model.get_conv_output_shape(images[0].to(device))\n",
    "print(output_shape)  # Print the output shape\n",
    "\n",
    "# Now update the input dimensions of your Linear layer and view function\n",
    "num_flat_features = output_shape[1] * output_shape[2] * output_shape[3]\n",
    "model.fc1 = nn.Linear(num_flat_features, 10)  # Update the Linear layer with the correct input dimensions\n",
    "\n",
    "def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = x.view(-1, num_flat_features)  # Update the view function with the correct dimensions\n",
    "    x = self.fc1(x)\n",
    "    return x\n",
    "# Train the model\n",
    "def train_model(num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[1].to(device), data[2].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "# Run the training function for a defined number of epochs\n",
    "train_model(num_epochs=5)\n",
    "\n",
    "# Here after training, you would save your model and/or evaluate it on your test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
