{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gabriel\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from models import model_classifier\n",
    "from utils_dir.utils import *\n",
    "import config\n",
    "\n",
    "if config.ESC_10:\n",
    "\timport dataset_ESC10 as dataset\n",
    "elif config.ESC_50:\n",
    "\timport dataset_ESC50 as dataset\n",
    "elif config.US8K:\n",
    "\timport dataset_US8K as dataset\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "model =torchvision.models.resnet50(pretrained=True).to(device)\n",
    "model.fc = nn.Sequential(nn.Identity())\n",
    "\n",
    "\n",
    "model = nn.DataParallel(model, device_ids=[0])\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "classifier = model_classifier.Classifier().to(device)\n",
    "\n",
    "\n",
    "train_loader, val_loader = dataset.create_generators()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "root = './results/'\n",
    "main_path = root + str(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M'))\n",
    "if not os.path.exists(main_path):\n",
    "\tos.mkdir(main_path)\n",
    "\n",
    "classifier_path = main_path + '/' + 'classifier'\n",
    "os.mkdir(classifier_path)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(model.parameters())+ list(classifier.parameters()), \n",
    "                             lr=config.lr, weight_decay=1e-3)\n",
    "\n",
    "scheduler = WarmUpExponentialLR(optimizer, cold_epochs= 0, warm_epochs= config.warm_epochs, gamma=config.gamma)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotEncoder(v):\n",
    "\tret_vec = torch.zeros(v.shape[0], config.class_numbers).to(device)\n",
    "\tfor s in range(v.shape[0]):\n",
    "\t\tret_vec[s][v[s]] = 1\n",
    "\treturn ret_vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_one_hot(input, target):\n",
    "\t_, labels = target.max(dim=1)\n",
    "\treturn nn.CrossEntropyLoss(weight=class_weights)(input, labels)\n",
    "\n",
    "###########################################################################################\n",
    "class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]).to(device)\n",
    "main_model = model.module if hasattr(model, 'module') else model\n",
    "###########################################################################################\n",
    "\n",
    "def train_crossEntropy():\n",
    "\tnum_epochs = 800\n",
    "\twith open(main_path + '/results.txt','w', 1) as output_file:\n",
    "\t\tmainModel_stopping = EarlyStopping(patience=300, verbose=True, log_path=main_path, output_file=output_file)\n",
    "\t\tclassifier_stopping = EarlyStopping(patience=300, verbose=False, log_path=classifier_path, output_file=output_file)\n",
    "\n",
    "\t\tprint('*****', file=output_file)\n",
    "\t\tprint('BASELINE', file=output_file)\n",
    "\t\tprint('transfer - augmentation on both waves and specs - 3 channels', file=output_file)\n",
    "\t\tif config.ESC_10:\n",
    "\t\t\tprint('ESC_10', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.train_folds, config.test_fold), file=output_file)\n",
    "\t\telif config.ESC_50:\n",
    "\t\t\tprint('ESC_50', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.train_folds, config.test_fold), file=output_file)\n",
    "\t\telif config.US8K:\n",
    "\t\t\tprint('US8K', file=output_file)\n",
    "\t\t\tprint('train folds are {} and test fold is {}'.format(config.us8k_train_folds, config.us8k_test_fold), file=output_file)\n",
    "\n",
    "\n",
    "\t\tprint('number of freq masks are {} and their max length is {}'.format(config.freq_masks, config.freq_masks_width), file=output_file)\n",
    "\t\tprint('number of time masks are {} and their max length is {}'.format(config.time_masks, config.time_masks_width), file=output_file)\n",
    "\t\tprint('*****', file=output_file)\n",
    "\t\n",
    "\n",
    "\n",
    "\t\tfor epoch in range(num_epochs):\n",
    "\t\t\tmodel.train()\n",
    "\t\t\tclassifier.train()\n",
    "        \n",
    "\t\t\ttrain_loss = []\n",
    "\t\t\ttrain_corrects = 0\n",
    "\t\t\ttrain_samples_count = 0\n",
    "        \n",
    "\t\t\tfor _, x, label in train_loader:\n",
    "\t\t\t\tloss = 0\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "            \n",
    "\t\t\t\tinp = x.float().to(device)\n",
    "\t\t\t\tlabel = label.to(device).unsqueeze(1)\n",
    "\t\t\t\tlabel_vec = hotEncoder(label)\n",
    "            \n",
    "\t\t\t\ty_rep = model(inp)\n",
    "\t\t\t\ty_rep = F.normalize(y_rep, dim=0)\n",
    "            \n",
    "\t\t\t\ty_pred = classifier(y_rep)\n",
    "            \n",
    "\t\t\t\tloss += cross_entropy_one_hot(y_pred, label_vec)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\ttrain_loss.append(loss.item() )\n",
    "\t\t\t\toptimizer.step()\n",
    "            \n",
    "\t\t\t\ttrain_corrects += (torch.argmax(y_pred, dim=1) == torch.argmax(label_vec, dim=1)).sum().item()\n",
    "\t\t\t\ttrain_samples_count += x.shape[0]\n",
    "        \n",
    "        \n",
    "\t\t\tval_loss = []\n",
    "\t\t\tval_corrects = 0\n",
    "\t\t\tval_samples_count = 0\n",
    "        \n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tclassifier.eval()\n",
    "        \n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tfor _, val_x, val_label in val_loader:\n",
    "\t\t\t\t\tinp = val_x.float().to(device)\n",
    "\t\t\t\t\tlabel = val_label.to(device)\n",
    "\t\t\t\t\tlabel_vec = hotEncoder(label)\n",
    "                \n",
    "\t\t\t\t\ty_rep = model(inp)\n",
    "\t\t\t\t\ty_rep = F.normalize(y_rep, dim=0)\n",
    "\n",
    "\t\t\t\t\ty_pred = classifier(y_rep)\n",
    "                \n",
    "\t\t\t\t\ttemp = cross_entropy_one_hot(y_pred, label_vec)\n",
    "\t\t\t\t\tval_loss.append(temp.item() )\n",
    "                \n",
    "\t\t\t\t\tval_corrects += (torch.argmax(y_pred, dim=1) == torch.argmax(label_vec, dim=1)).sum().item() \n",
    "\t\t\t\t\tval_samples_count += val_x.shape[0]\n",
    "        \n",
    "        \n",
    "\t\t\tscheduler.step()\n",
    "        \n",
    "\t\t\ttrain_acc = train_corrects / train_samples_count\n",
    "\t\t\tval_acc = val_corrects / val_samples_count\n",
    "\t\t\tprint('\\n', file=output_file)\n",
    "\t\t\tprint(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \"Loss: {:.4f}...\".format(np.mean(train_loss)),\n",
    "\t\t\t\t\"Val Loss: {:.4f}\".format(np.mean(val_loss)), file=output_file)\n",
    "\t\t\tprint('train_acc is {:.4f} and val_acc is {:.4f}'.format(train_acc, val_acc), file=output_file)\n",
    "\t\t\tmainModel_stopping(-val_acc, main_model, epoch+1)\n",
    "\t\t\tclassifier_stopping(-val_acc, classifier, epoch+1)\n",
    "\t\t\tif mainModel_stopping.early_stop:\n",
    "\t\t\t\tprint(\"Early stopping\", file=output_file)\n",
    "\t\t\t\treturn\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttrain_crossEntropy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
